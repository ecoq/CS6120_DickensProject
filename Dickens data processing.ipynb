{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed8c524-be1c-48f5-9168-b8fe3759d926",
   "metadata": {},
   "source": [
    "Elizabeth Coquillette - CS6120 Natural Language Processing\n",
    "NUID 002453121\n",
    "Final Project Code (Part 1 of 2 - Processing Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce3c04e-615a-4ce2-accc-aadc2cd050e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL THESE PACKAGES IF NOT YET DONE\n",
    "\n",
    "#!pip install lexicalrichness\n",
    "#!pip install textstat\n",
    "#!pip install vaderSentiment\n",
    "# nltk.download('universal_tagset')'\n",
    "\n",
    "\n",
    "# LOAD DEPENDENCIES\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, math, statistics, time, traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from lexicalrichness import LexicalRichness\n",
    "import textstat\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from functools import lru_cache\n",
    "from scipy.stats import entropy as shannon_entropy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag.mapping import map_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe28656-732b-4b80-89c5-a0a8e6319980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART ONE - CHAPTER SEGMENTATION\n",
    "\n",
    "# Regex identification of what counts as a chapter label\n",
    "#    Accepts only CHAPTER or Chapter followed by digits or roman numerals\n",
    "#    Also accepts the special case of \"Chapter the Last\" for a particular book\n",
    "CHAPTER_LINE = re.compile(\n",
    "    r\"\"\"(?mi)                      # multiline + case-insensitive\n",
    "        ^\\s*chapter                # starts with 'chapter' (any case)\n",
    "        \\s+\n",
    "        (?:[IVXLCDM]+|\\d+|the\\s+last)   # Roman numerals OR digits OR 'the last'\n",
    "        \\b\n",
    "        [^\\n]*$                    # rest of heading line (optional)\n",
    "    \"\"\",\n",
    "    re.VERBOSE,\n",
    ")\n",
    "\n",
    "# Identify chapter class\n",
    "@dataclass\n",
    "class Chapter:\n",
    "    index: int\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "def segment_into_chapters(text):\n",
    "    \"\"\"\n",
    "    Split the text into chapters using CHAPTER_LINE.\n",
    "    Returns a list of Chapter objects.\n",
    "    If no headings are found, returns one 'FULL TEXT' chapter.\n",
    "    \"\"\"\n",
    "    # Find matched headings\n",
    "    spans = list(CHAPTER_LINE.finditer(text))\n",
    "   \n",
    "    chapters = []\n",
    "    \n",
    "    # Take each chapter and save it individually\n",
    "    for i, m in enumerate(spans):\n",
    "        start = m.start()\n",
    "\n",
    "        # End at the start of next match, or end of text if this is the last chapter\n",
    "        end = spans[i + 1].start() if i + 1 < len(spans) else len(text)\n",
    "        title = text[m.start(): m.end()].strip().replace(\"\\n\", \" \")\n",
    "        body  = text[m.end(): end].strip()\n",
    "        if body:\n",
    "            chapters.append(Chapter(len(chapters) + 1, title, body))\n",
    "    return chapters or [Chapter(1, \"FULL TEXT\", text.strip())]\n",
    "\n",
    "    # No matches → single chapter fallback\n",
    "    if not spans:\n",
    "        return [Chapter(1, \"FULL TEXT\", text.strip())]\n",
    "\n",
    "    # Slice text between matched headings\n",
    "    chapters: List[Chapter] = []\n",
    "    \n",
    "    # Take each chapter and save it individually\n",
    "    for i, m in enumerate(spans):\n",
    "        start = m.start()\n",
    "        # End at the start of next match, or end of text if this is the last chapter\n",
    "        end = spans[i + 1].start() if i + 1 < len(spans) else len(text)\n",
    "        title = text[m.start(): m.end()].strip().replace(\"\\n\", \" \")\n",
    "        body  = text[m.end(): end].strip()\n",
    "        if body:\n",
    "            chapters.append(Chapter(len(chapters) + 1, title, body))\n",
    "\n",
    "    return chapters or [Chapter(1, \"FULL TEXT\", text.strip())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a367f2-ff41-4231-a8be-013c71385704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART TWO - CHAPTER TEXT PROCESSING\n",
    "\n",
    "# Mapping table for universal POS tags\n",
    "_UNI2WN = {\"NOUN\": \"n\", \"VERB\": \"v\", \"ADJ\": \"a\", \"ADV\": \"r\"}\n",
    "\n",
    "# Create lemmatizer & Vader\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "_VADER = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# All the features we are going to save\n",
    "@dataclass\n",
    "class ChapterFeatures:\n",
    "    # Core identifiers\n",
    "    book_id: str\n",
    "    chapter_index: int\n",
    "    chapter_title: str\n",
    "    \n",
    "    # Basic structure stats\n",
    "    n_sentences: int\n",
    "    n_tokens: int                # all tokens (incl. punctuation/numerals)\n",
    "    n_tokens_alpha: int          # alphabetic tokens we POS-tag/lemmatize\n",
    "\n",
    "    # Basic word & sentence length measurements\n",
    "    mean_sentence_len_tokens: float\n",
    "    var_sentence_len: float\n",
    "    mean_word_len: float\n",
    "    var_word_len: float\n",
    "    \n",
    "    # POS distributions (counts)\n",
    "    pos_counts_universal: Dict[str, int]\n",
    "    pos_counts_penn: Dict[str, int]\n",
    "    \n",
    "    # Lexical richness\n",
    "    ttr: float                   # type-token ratio over alpha tokens (lowercased)\n",
    "    mtld: float                  # MTLD (lexicalrichness) over alpha tokens (lowercased)\n",
    "    \n",
    "    # Readability\n",
    "    flesch_reading_ease: float\n",
    "    flesch_kincaid_grade: float\n",
    "    \n",
    "    # Lemma variety\n",
    "    n_unique_lemmas: int\n",
    "    \n",
    "    # QC sample\n",
    "    sample_sentences: List[str]\n",
    "\n",
    "    # Entropy\n",
    "    word_entropy_bits: float\n",
    "\n",
    "    # Vader sentiment analysis\n",
    "    vader_compound: float\n",
    "    vader_pos: float\n",
    "    vader_neu: float\n",
    "    vader_neg: float\n",
    "\n",
    "def _lemmatize_tokens(tokens_alpha, univ_tags):\n",
    "    \"\"\"Lemmatize alphabetic tokens using Universal POS to guide WordNet.\"\"\"\n",
    "    lemmas = []\n",
    "    for tok, upos in zip(tokens_alpha, univ_tags):\n",
    "        # Convert Universal POS tag to the WordNet format\n",
    "        wn_pos = _UNI2WN.get(upos, \"n\")  # default to noun\n",
    "\n",
    "        # Lemmatize token with the right tag\n",
    "        lemmas.append(lemmatizer.lemmatize(tok, pos=wn_pos))\n",
    "    return lemmas\n",
    "\n",
    "def _compute_lexical_richness(tokens_alpha):\n",
    "    \"\"\" Returns (TTR, MTLD). TTR is unique/total over lowercased alphabetic tokens; MTLD uses lexical richness. \"\"\"\n",
    "    # If no tokens, can't compute\n",
    "    if not tokens_alpha:\n",
    "        return (math.nan, math.nan)\n",
    "\n",
    "    # Convert tokens to lowercase\n",
    "    lower = [t.lower() for t in tokens_alpha]\n",
    "\n",
    "    # Number of unique types\n",
    "    types = len(set(lower))\n",
    "\n",
    "    # Type-token ratio (unique words/total words)\n",
    "    ttr = types / len(lower)\n",
    "\n",
    "    # Initialize lexical richness with the text string\n",
    "    lr = LexicalRichness(\" \".join(lower))\n",
    "\n",
    "    # Compute MTLD with the standard threshold (0.72)\n",
    "    mtld = lr.mtld(threshold=0.72)\n",
    "\n",
    "    return (ttr, mtld)\n",
    "\n",
    "def _compute_readability(text):\n",
    "    \"\"\"Returns Flesch Reading Ease & Flesch-Kincaid Grade\"\"\"\n",
    "    fre = float(textstat.flesch_reading_ease(text))\n",
    "    fkg = float(textstat.flesch_kincaid_grade(text))\n",
    "    return (fre, fkg)\n",
    "\n",
    "def _word_entropy_bits(tokens_lower):\n",
    "    \"\"\" Compute hannon entropy H(X) over lowercased word tokens \"\"\"\n",
    "    # Frequency of each unique token\n",
    "    counts = np.fromiter(\n",
    "        (c for c in Counter(tokens_lower).values()),\n",
    "        dtype=float\n",
    "    )\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return math.nan\n",
    "\n",
    "    # Probabilities\n",
    "    probs = counts / total\n",
    "\n",
    "    # SciPy entropy with base=2 gives bits\n",
    "    return float(shannon_entropy(probs, base=2))\n",
    "\n",
    "def process_segment(chapter_or_chapters, book_id, keep_sentence_texts=True, is_full_book=False):\n",
    "    \"\"\"\n",
    "    Do full processing e a single chapter or the entire book.\n",
    "\n",
    "    Parameters -- chapter_or_chapters depends on whether is_full_book=True (multiple chapters) or False\n",
    "        (single chapter). \n",
    "    Optional to store sample sentences using keep_sentence_texts = True.\n",
    "\n",
    "    Returns chapterFeatures summary row\n",
    "    \"\"\"\n",
    "\n",
    "    # If we are using full book, adjust settings accordingly\n",
    "    if is_full_book:\n",
    "        # Concatenate the *clean* chapter texts\n",
    "        full_text = \"\\n\\n\".join(ch.text for ch in chapter_or_chapters if ch.text.strip())\n",
    "        text = full_text\n",
    "        chapter_index = 0\n",
    "        chapter_title = \"__FULL_BOOK__\"\n",
    "        keep_samples = False  # usually omit samples for full-book\n",
    "    # Otherwise adjust settings for chapters\n",
    "    else:\n",
    "        chapter = chapter_or_chapters\n",
    "        text = chapter.text\n",
    "        chapter_index = chapter.index\n",
    "        chapter_title = chapter.title\n",
    "        keep_samples = keep_sentence_texts\n",
    "\n",
    "    # Sentence segmentation\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    # Word tokenization per sentence and flatten\n",
    "    sent_tokens = [word_tokenize(s) for s in sents]\n",
    "    tokens = [t for sent in sent_tokens for t in sent if t.strip() != \"\"]\n",
    "    tokens_alpha = [t for t in tokens if t.isalpha()]  # focus on alphabetic for POS/lemmas/richness\n",
    "\n",
    "    # POS tagging (Penn), then map to Universal\n",
    "    penn_pairs = nltk.pos_tag(tokens_alpha)                      # [('fox','NN'), ('jumps','VBZ'), ...]\n",
    "    penn_tags = [tag for _, tag in penn_pairs]                   # ['NN','VBZ', ...]\n",
    "    univ_tags = [map_tag(\"en-ptb\", \"universal\", t) for t in penn_tags]  # ['NOUN','VERB', ...]\n",
    "\n",
    "    # Count penn tagset\n",
    "    pos_counts_penn: Dict[str, int] = {}\n",
    "    for tag in penn_tags:\n",
    "        pos_counts_penn[tag] = pos_counts_penn.get(tag, 0) + 1\n",
    "\n",
    "    # Count universal tagset\n",
    "    pos_counts_universal: Dict[str, int] = {}\n",
    "    for tag in univ_tags:\n",
    "        pos_counts_universal[tag] = pos_counts_universal.get(tag, 0) + 1\n",
    "\n",
    "    # Lemmas + lemma variety \n",
    "    lemmas = _lemmatize_tokens(tokens_alpha, univ_tags)\n",
    "    n_unique_lemmas = len(set(lemmas))\n",
    "\n",
    "    # Lexical richness (TTR + MTLD)\n",
    "    ttr, mtld = _compute_lexical_richness(tokens_alpha)\n",
    "\n",
    "    # Readability (Flesch) on raw text\n",
    "    fre, fkg = _compute_readability(text)\n",
    "\n",
    "    # Basic stats built from the same tokenization\n",
    "    sent_lengths = [len(st) for st in sent_tokens]\n",
    "\n",
    "    # Whole-segment tokens\n",
    "    word_lengths = [len(t) for t in tokens_alpha]  # character lengths of alphabetic tokens\n",
    "\n",
    "    # Basic counts\n",
    "    n_sentences = len(sent_lengths)\n",
    "    n_tokens = sum(sent_lengths)\n",
    "    n_tokens_alpha = len(tokens_alpha)\n",
    "\n",
    "    # Mean sentence length in tokens\n",
    "    mean_sentence_len_tokens = float(np.mean(sent_lengths)) if n_sentences else np.nan\n",
    "    sample = sents[:3] if keep_samples else []\n",
    "\n",
    "    # Sentence-length variance (population)\n",
    "    var_sentence_len = float(np.var(sent_lengths, ddof=0)) if n_sentences >= 2 else np.nan\n",
    "\n",
    "    # Mean & variance of word length\n",
    "    mean_word_len = float(np.mean(word_lengths)) if word_lengths else np.nan\n",
    "    var_word_len  = float(np.var(word_lengths, ddof=0)) if len(word_lengths) >= 2 else (0.0 if word_lengths else np.nan)\n",
    "\n",
    "    # Entropy over lowercased alphabetic tokens\n",
    "    lower_alpha = [t.lower() for t in tokens_alpha]\n",
    "    word_ent = float(_word_entropy_bits(lower_alpha)) if lower_alpha else np.nan\n",
    "\n",
    "    # Sentiment (VADER) on raw text\n",
    "    if '_VADER' in globals() and _VADER is not None and len(text.split()) >= 3:\n",
    "        vs = _VADER.polarity_scores(text)\n",
    "        vader_compound = float(vs.get(\"compound\", math.nan))\n",
    "        vader_pos = float(vs.get(\"pos\", math.nan))\n",
    "        vader_neu = float(vs.get(\"neu\", math.nan))\n",
    "        vader_neg = float(vs.get(\"neg\", math.nan))\n",
    "    else:\n",
    "        vader_compound = vader_pos = vader_neu = vader_neg = math.nan\n",
    "\n",
    "    # Return the unified feature row\n",
    "    return ChapterFeatures(\n",
    "        book_id=book_id,\n",
    "        chapter_index=chapter_index,\n",
    "        chapter_title=chapter_title,\n",
    "        n_sentences=n_sentences,\n",
    "        n_tokens=n_tokens,\n",
    "        n_tokens_alpha=n_tokens_alpha,\n",
    "        mean_sentence_len_tokens=mean_sentence_len_tokens,\n",
    "        pos_counts_universal=dict(sorted(pos_counts_universal.items())),\n",
    "        pos_counts_penn=dict(sorted(pos_counts_penn.items())),\n",
    "        ttr=float(round(ttr, 4)) if not math.isnan(ttr) else math.nan,\n",
    "        mtld=float(round(mtld, 4)) if (mtld is not None and not math.isnan(mtld)) else math.nan,\n",
    "        flesch_reading_ease=fre,\n",
    "        flesch_kincaid_grade=fkg,\n",
    "        n_unique_lemmas=n_unique_lemmas,\n",
    "        sample_sentences=sample,\n",
    "        var_sentence_len=var_sentence_len,\n",
    "        mean_word_len=mean_word_len,\n",
    "        var_word_len=var_word_len,\n",
    "        vader_compound=vader_compound,\n",
    "        vader_pos=vader_pos,\n",
    "        vader_neu=vader_neu,\n",
    "        vader_neg=vader_neg,\n",
    "        word_entropy_bits=word_ent,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b322b14-547e-46c4-a820-68a7d3a4e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART THREE - ADDITIONAL HELPER FUNCTIONS\n",
    "\n",
    "def expand_pos_counts_and_props(df):\n",
    "    \"\"\"\n",
    "    Expand POS dictionary columns into wide numeric columns AND add per-chapter POS *proportion* columns (relative frequencies).\n",
    "    This is useful data restructuring for later use in analysis.\n",
    "    \"\"\"\n",
    "    # Sub-part 1 - Expand dicts to wide numeric columns\n",
    "    \n",
    "    # Universal POS counts\n",
    "    if \"pos_counts_universal\" in df.columns:\n",
    "        # Convert dict to columns; missing keys → NaN → fill with 0 → int\n",
    "        pos_univ = (\n",
    "            df[\"pos_counts_universal\"]\n",
    "            .apply(lambda d: pd.Series(d))\n",
    "            .fillna(0)\n",
    "            .astype(int)\n",
    "            .add_prefix(\"pos_univ_\")  # clear prefix to avoid name clashes\n",
    "        )\n",
    "        # Replace the dict column with expanded columns\n",
    "        df = pd.concat([df.drop(columns=[\"pos_counts_universal\"]), pos_univ], axis=1)\n",
    "\n",
    "    # Penn Treebank POS counts\n",
    "    if \"pos_counts_penn\" in df.columns:\n",
    "        pos_penn = (\n",
    "            df[\"pos_counts_penn\"]\n",
    "            .apply(lambda d: pd.Series(d))\n",
    "            .fillna(0)\n",
    "            .astype(int)\n",
    "            .add_prefix(\"pos_penn_\")\n",
    "        )\n",
    "        df = pd.concat([df.drop(columns=[\"pos_counts_penn\"]), pos_penn], axis=1)\n",
    "\n",
    "    # Sub-part 2 - Add proportion (relative frequency) cols\n",
    "    \n",
    "    # Denominator = number of alphabetic tokens tagged; replace 0 with NA to avoid div-by-zero\n",
    "    denom = df[\"n_tokens_alpha\"].replace(0, pd.NA)\n",
    "\n",
    "    # For Universal tags: create pos_univ_prop_<TAG> for each count column\n",
    "    univ_count_cols = [\n",
    "        c for c in df.columns\n",
    "        if c.startswith(\"pos_univ_\") and not c.startswith(\"pos_univ_prop_\")\n",
    "    ]\n",
    "    for c in univ_count_cols:\n",
    "        tag = c[len(\"pos_univ_\"):]  # extract the tag name\n",
    "        df[f\"pos_univ_prop_{tag}\"] = (df[c] / denom).astype(float)\n",
    "\n",
    "    # For Penn tags: create pos_penn_prop_<TAG> for each count column\n",
    "    penn_count_cols = [\n",
    "        c for c in df.columns\n",
    "        if c.startswith(\"pos_penn_\") and not c.startswith(\"pos_penn_prop_\")\n",
    "    ]\n",
    "    for c in penn_count_cols:\n",
    "        tag = c[len(\"pos_penn_\"):]\n",
    "        df[f\"pos_penn_prop_{tag}\"] = (df[c] / denom).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Timestamp helper function for tracking performance\n",
    "def _now():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "# Helper function to save dataframe to csv\n",
    "def save_output_csv(df, filename):\n",
    "    filename = Path(filename)\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382feb88-8ad1-4318-a184-67651dc81852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART FOUR - MAIN PROCESSING FUNCTIONS\n",
    "\n",
    "def process_book_to_df(path, book_id, full_book = True, keep_sentence_texts = False):\n",
    "    \"\"\"\n",
    "    Read the file at `path` and fully process chapters and optionally FULL_BOOK summary row (chapter index=0).\n",
    "    Returns wide dataframe ready for downstream expansion (POS props) and saving.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read & clean\n",
    "    p = Path(path)\n",
    "    print(f\"    [{_now()}] read_text -> {p}\")\n",
    "    text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # Segment into chapters\n",
    "    print(f\"    [{_now()}] segment_chapters\")\n",
    "    chapters = segment_into_chapters(text)\n",
    "    print(f\"    [{_now()}] chapters_found={len(chapters)}\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # If processing is being run on full book, do that\n",
    "    if full_book:\n",
    "        print(f\"    [{_now()}] full_book start\")\n",
    "        row0 = process_segment(chapters, book_id, is_full_book=True, keep_sentence_texts=False)\n",
    "        rows.append(row0)\n",
    "        print(f\"    [{_now()}] full_book done\")\n",
    "    else:\n",
    "        print(\"Full book option not enabled\")\n",
    "\n",
    "    # Run processing on chapters\n",
    "    print(f\"    [{_now()}] chapter_loop start (n={len(chapters)})\")\n",
    "    import time\n",
    "    for i, ch in enumerate(chapters, 1):\n",
    "        # Timing settings to track progress\n",
    "        t0 = time.time()\n",
    "        if i == 1 or i % 5 == 0:\n",
    "            print(f\"    [{_now()}]   chapter {i}/{len(chapters)} — start\")\n",
    "\n",
    "        # Actually process the segment\n",
    "        row = process_segment(ch, book_id, is_full_book=False, keep_sentence_texts=True)\n",
    "        rows.append(row)\n",
    "\n",
    "        # Timing settings to track progress\n",
    "        dt = time.time() - t0\n",
    "        if i == 1 or i % 5 == 0 or dt > 2.0:\n",
    "            print(f\"    [{_now()}]   chapter {i} done in {dt:.2f}s\")\n",
    "\n",
    "    # Store in dataframe\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Accounts for some older code with different variable names\n",
    "    if \"var_sentence_len_tokens\" in df.columns and \"var_sentence_len\" not in df.columns:\n",
    "        df[\"var_sentence_len\"] = df[\"var_sentence_len_tokens\"]\n",
    "    if \"mean_sentence_len_tokens\" in df.columns and \"mean_sentence_len\" not in df.columns:\n",
    "        df[\"mean_sentence_len\"] = df[\"mean_sentence_len_tokens\"]\n",
    "\n",
    "    # Expand POS dicts + add proportions (skips if dict columns are absent)\n",
    "    if \"pos_counts_universal\" in df.columns or \"pos_counts_penn\" in df.columns:\n",
    "        print(\"expanding pos_counts\")\n",
    "        df = expand_pos_counts_and_props(df)\n",
    "\n",
    "    # Column ordering\n",
    "    front = [\n",
    "        \"book_id\",                    # ID or name of the book\n",
    "        \"chapter_index\",              # Chapter number (0 = full book summary row, if present)\n",
    "        \"chapter_title\",              # Title/heading of the chapter\n",
    "        \"n_sentences\",                # Number of sentences in chapter\n",
    "        \"n_tokens\",                   # Total tokens (including punctuation, numbers, etc.)\n",
    "        \"n_tokens_alpha\",             # Tokens containing only alphabetic characters\n",
    "        \"ttr\",                        # Type-Token Ratio (lexical diversity)\n",
    "        \"mtld\",                       # Measure of Textual Lexical Diversity\n",
    "        \"flesch_reading_ease\",        # Flesch Reading Ease score\n",
    "        \"flesch_kincaid_grade\",       # Flesch-Kincaid Grade Level\n",
    "        \"n_unique_lemmas\",            # Unique lemmatized word forms\n",
    "        \"mean_sentence_len\",          # Mean sentence length\n",
    "        \"var_sentence_len\",           # Variance sentence length\n",
    "        \"mean_word_len\",              # Mean word length\n",
    "        \"var_word_len\",               # Variance word length\n",
    "        \"word_entropy_bits\",          # Word entropy\n",
    "        \"vader_compound\",             # Vader sentiment analysis\n",
    "        \"vader_pos\", \n",
    "        \"vader_neu\", \n",
    "        \"vader_neg\",\n",
    "        \"pos_counts_universal\",\n",
    "        \"pos_counts_penn\"\n",
    "    ]\n",
    "\n",
    "    # Reorder columns\n",
    "    existing_front = [c for c in front if c in df.columns]\n",
    "    other_cols = [c for c in df.columns if c not in existing_front]\n",
    "    \n",
    "    # Keep sample sentences last if present\n",
    "    if \"sample_sentences\" in other_cols:\n",
    "        other_cols = [c for c in other_cols if c != \"sample_sentences\"] + [\"sample_sentences\"]\n",
    "\n",
    "    # Reindex and sort\n",
    "    df = df.reindex(columns=existing_front + other_cols)\n",
    "    df = df.sort_values([\"chapter_index\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def batch_process_books(books, full_book=True):\n",
    "    \"\"\"\n",
    "    Process multiple novels and return:\n",
    "      - df_all: concatenated chapter-level tables plus a FULL_BOOK row\n",
    "      - df_summary: subset with only FULL_BOOK rows\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "\n",
    "    # Loop over each book specification\n",
    "    for spec in books:\n",
    "        p = Path(spec[\"path\"])\n",
    "        book_id = spec[\"book_id\"]\n",
    "        author = spec.get(\"author\", None)\n",
    "\n",
    "        print(f\"[{_now()}] → START {book_id} ({p})\")\n",
    "\n",
    "        try:\n",
    "            # Process the book with all features enabled except keeping sentence texts\n",
    "            df = process_book_to_df(p, book_id=book_id, full_book=full_book, keep_sentence_texts=False)\n",
    "\n",
    "            # Add author if given\n",
    "            if author is not None:\n",
    "                df.insert(0, \"author\", author)\n",
    "\n",
    "            # Add this book to the frames\n",
    "            frames.append(df)\n",
    "            print(f\"✓ Processed {book_id} ({p}) with {len(df)} rows\")\n",
    "\n",
    "        # Print exception if one of the books doesn't work and carry on with the others\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed on {book_id} ({p}): {e}\")\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(\"No books processed successfully\")\n",
    "\n",
    "    # Combine all into one\n",
    "    df_all = pd.concat(frames, ignore_index=True, sort=False)\n",
    "\n",
    "    # Extract just the FULL_BOOK rows for summary\n",
    "    df_summary = df_all[df_all[\"chapter_index\"] == 0].copy()\n",
    "\n",
    "    return df_all, df_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28539d12-2769-44e9-bc26-441dd1861719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHORTENED VERSION ENABLED\n",
      "[20:07:51] → START TheOldCuriosityShop (texts/TheOldCuriosityShop.txt)\n",
      "    [20:07:51] read_text -> texts/TheOldCuriosityShop.txt\n",
      "    [20:07:51] segment_chapters\n",
      "    [20:07:51] chapters_found=73\n",
      "Full book option not enabled\n",
      "    [20:07:51] chapter_loop start (n=73)\n",
      "    [20:07:51]   chapter 1/73 — start\n",
      "    [20:07:51]   chapter 1 done in 0.35s\n",
      "    [20:07:52]   chapter 5/73 — start\n",
      "    [20:07:52]   chapter 5 done in 0.09s\n",
      "    [20:07:52]   chapter 10/73 — start\n",
      "    [20:07:52]   chapter 10 done in 0.08s\n",
      "    [20:07:53]   chapter 15/73 — start\n",
      "    [20:07:53]   chapter 15 done in 0.16s\n",
      "    [20:07:54]   chapter 20/73 — start\n",
      "    [20:07:54]   chapter 20 done in 0.06s\n",
      "    [20:07:54]   chapter 25/73 — start\n",
      "    [20:07:54]   chapter 25 done in 0.13s\n",
      "    [20:07:55]   chapter 30/73 — start\n",
      "    [20:07:55]   chapter 30 done in 0.07s\n",
      "    [20:07:55]   chapter 35/73 — start\n",
      "    [20:07:56]   chapter 35 done in 0.19s\n",
      "    [20:07:56]   chapter 40/73 — start\n",
      "    [20:07:56]   chapter 40 done in 0.13s\n",
      "    [20:07:57]   chapter 45/73 — start\n",
      "    [20:07:57]   chapter 45 done in 0.11s\n",
      "    [20:07:57]   chapter 50/73 — start\n",
      "    [20:07:57]   chapter 50 done in 0.16s\n",
      "    [20:07:58]   chapter 55/73 — start\n",
      "    [20:07:58]   chapter 55 done in 0.08s\n",
      "    [20:07:59]   chapter 60/73 — start\n",
      "    [20:07:59]   chapter 60 done in 0.19s\n",
      "    [20:07:59]   chapter 65/73 — start\n",
      "    [20:07:59]   chapter 65 done in 0.07s\n",
      "    [20:08:00]   chapter 70/73 — start\n",
      "    [20:08:00]   chapter 70 done in 0.08s\n",
      "expanding pos_counts\n",
      "✓ Processed TheOldCuriosityShop (texts/TheOldCuriosityShop.txt) with 73 rows\n"
     ]
    }
   ],
   "source": [
    "# PART FIVE - RUNNING THE FUNCTIONS\n",
    "\n",
    "# List of books to process\n",
    "# Include author = Dickens field to allow for adding new authors later\n",
    "books = [\n",
    "    {\"path\": \"texts/TheOldCuriosityShop.txt\", \"book_id\": \"TheOldCuriosityShop\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/PickwickPapers.txt\", \"book_id\": \"PickwickPapers\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/OurMutualFriend.txt\", \"book_id\": \"OurMutualFriend\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/OliverTwist.txt\", \"book_id\": \"OliverTwist\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/NicholasNickelby.txt\", \"book_id\": \"NicholasNickelby\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/MartinChuzzlewit.txt\", \"book_id\": \"MartinChuzzlewit\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/LittleDorrit.txt\", \"book_id\": \"LittleDorrit\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/HardTimes.txt\", \"book_id\": \"HardTimes\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/GreatExpectations.txt\", \"book_id\": \"GreatExpectations\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/DombeyAndSon.txt\", \"book_id\": \"DombeyAndSon\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/DavidCopperfield.txt\", \"book_id\": \"DavidCopperfield\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/BleakHouse.txt\", \"book_id\": \"BleakHouse\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/BarnabyRudge.txt\", \"book_id\": \"BarnabyRudge\", \"author\": \"Dickens\"},\n",
    "    {\"path\": \"texts/ATaleOfTwoCities.txt\", \"book_id\": \"ATaleOfTwoCities\", \"author\": \"Dickens\"},\n",
    "]\n",
    "\n",
    "# Option for shortened version of books for testing purposes\n",
    "#books = [{\"path\": \"texts/TheOldCuriosityShop.txt\", \"book_id\": \"TheOldCuriosityShop\", \"author\": \"Dickens\"} ]\n",
    "#print(\"SHORTENED VERSION ENABLED\")\n",
    "\n",
    "# Process\n",
    "df_all, df_summary = batch_process_books(books, full_book=False)\n",
    "\n",
    "# Save combined outputs\n",
    "save_output_csv(df_all, filename=\"data/processed/ALL_books_chapters_plus_fullbook.csv\")\n",
    "save_output_csv(df_summary, filename=\"data/processed/ALL_books_fullbook_summary.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcc049-a339-4164-be16-f8b2e8c74abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
